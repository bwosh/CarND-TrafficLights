{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLA-34 in Keras with HSwish as option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/bmatuszewki/Projects/CND/ccc/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, BatchNormalization, Input, MaxPool2D\n",
    "from keras.activations import relu\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "    \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLU, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ReLU, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.relu(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class HSwish(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HSwish, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(HSwish, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        six = K.ones_like(x)*6\n",
    "        return x* K.minimum(K.relu(x+3),six)/6\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree():\n",
    "    def __init__(self, name):\n",
    "        self.root = None\n",
    "        self.tree1 = None\n",
    "        self.tree2 = None\n",
    "        self.level_root = None\n",
    "        self.root_dim = None\n",
    "        self.downsample = None\n",
    "        self.project = None\n",
    "        self.levels = None\n",
    "        \n",
    "        self.name = name\n",
    "        \n",
    "    def __print_indent(self, indent, text, end='\\n'):\n",
    "        print( \"\".join([' ']*indent), end='')\n",
    "        print(text, end = end)\n",
    "              \n",
    "    def print(self, indent = 0):\n",
    "        self.__print_indent(1 if indent>0 else 0, \"[NODE: \"+str(self.name), end=' ')\n",
    "        self.__print_indent(0, \"lr:\"+str(self.level_root), end=' ')\n",
    "        self.__print_indent(0, \"rdim:\"+str(self.root_dim), end=' ')\n",
    "        self.__print_indent(0, \"downs: \"+str(self.downsample==None), end=' ')\n",
    "        self.__print_indent(0, \"proj: \"+str(self.project==None), end=' ')\n",
    "        self.__print_indent(0, \"levs: \"+str(self.levels), end=']\\n')\n",
    "        \n",
    "        if self.root is None:\n",
    "              self.__print_indent(indent+2, \"R1> None\")\n",
    "        else:\n",
    "              self.__print_indent(indent+2, \"R1> conv,bn,act \")\n",
    "              \n",
    "        if isinstance(self.tree1,Tree):\n",
    "            self.__print_indent(indent+2, \"T1>\", end='')\n",
    "            self.tree1.print(indent=indent+4)\n",
    "        elif self.tree1 is not None:\n",
    "            self.__print_indent(indent+2, \"T1> conv,bn\")\n",
    "\n",
    "        if isinstance(self.tree2,Tree):\n",
    "            self.__print_indent(indent+2, \"T2>\", end='')\n",
    "            self.tree2.print(indent=indent+4)\n",
    "        elif self.tree2 is not None:\n",
    "            self.__print_indent(indent+2, \"T2> conv,bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DlaKeras():\n",
    "    def __init__(self,  \n",
    "                 levels=[1, 1, 1, 2, 2, 1],\n",
    "                 planes=[16, 32, 64, 128, 256, 512],\n",
    "                 activation_function=\"relu\"):\n",
    "        self.norm_axis = -1\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.base_layer = self._make_simple_block(planes[0], kernel_size=7)\n",
    "        self.level0 = self._make_conv_level(planes[0], levels[0], stride=1)\n",
    "        self.level1 = self._make_conv_level(planes[1], levels[1], stride=2)\n",
    "        \n",
    "        self.level2 = self._make_tree(\"L2_\", levels[2], planes[1], planes[2], level_root=False)\n",
    "        self.level3 = self._make_tree(\"L3_\",levels[3], planes[2], planes[3])\n",
    "        self.level4 = self._make_tree(\"L4_\",levels[4], planes[3], planes[4])\n",
    "        self.level5 = self._make_tree(\"L5_\",levels[5], planes[4], planes[5])\n",
    "        \n",
    "    def _make_tree(self, name, levels, in_planes, planes, stride=2, level_root=True, root_dim=0):\n",
    "        result = Tree(name)\n",
    "        \n",
    "        if root_dim == 0:\n",
    "            root_dim = 2 * planes\n",
    "        if level_root:\n",
    "            root_dim += in_planes\n",
    "        \n",
    "        if levels==1:\n",
    "            result.tree1 = self._make_basic_block(planes, stride)\n",
    "            result.tree2 = self._make_basic_block(planes, 1)\n",
    "        else:\n",
    "            result.tree1 = self._make_tree(name+\"1\",levels-1, in_planes, planes, stride=stride)\n",
    "            result.tree2 = self._make_tree(name+\"2\",levels-1, planes, planes, stride=1, root_dim=root_dim + planes)\n",
    "            \n",
    "        if levels == 1:\n",
    "            result.root = self._make_root(root_dim, planes)   # TODO minor: remove root_dim ?\n",
    "        \n",
    "        result.level_root = level_root\n",
    "        result.root_dim = root_dim\n",
    "        result.levels = levels\n",
    "        if stride > 1:\n",
    "            result.downsample = MaxPool2D(pool_size=(stride, stride), strides=(stride, stride), padding = 'same')\n",
    "            \n",
    "        if in_planes != planes:\n",
    "            result.project = []\n",
    "            result.project.append(Conv2D(filters=planes, kernel_size=1, padding='same', strides=(1,1), use_bias=False))\n",
    "            result.project.append(BatchNormalization(axis=self.norm_axis))    \n",
    "\n",
    "        return result\n",
    "\n",
    "    def _make_root(self, in_planes, planes):\n",
    "        return self._make_simple_block(planes)\n",
    "    \n",
    "    def _make_basic_block(self, planes, stride=1):\n",
    "        layers=[]\n",
    "        \n",
    "        conv1 = Conv2D(filters=planes, kernel_size=3, padding='same', strides=(stride,stride), use_bias=False)\n",
    "        bn1 = BatchNormalization(axis=self.norm_axis)\n",
    "        if self.activation_function == \"hswish\":\n",
    "            activation = HSwish()\n",
    "        else:\n",
    "            activation = ReLU()\n",
    "        conv2 = Conv2D(filters=planes, kernel_size=3, padding='same', strides=(1,1), use_bias=False)\n",
    "        bn2 = BatchNormalization(axis=self.norm_axis)\n",
    "        \n",
    "        layers = [conv1,bn1,activation, conv2, bn2]\n",
    "        return layers\n",
    "    \n",
    "    def _make_simple_block(self, planes, kernel_size=3, stride = 1):\n",
    "        \n",
    "        conv = Conv2D(filters=planes, kernel_size=kernel_size, padding='same', strides=(stride,stride), use_bias=False)\n",
    "        bn = BatchNormalization(axis=self.norm_axis)\n",
    "        if self.activation_function == \"hswish\":\n",
    "            activation = HSwish()\n",
    "        else:\n",
    "            activation = ReLU()\n",
    "        \n",
    "        layers = [conv,bn,activation]\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def _make_conv_level(self, planes, levels, stride = 1):\n",
    "        layers = []\n",
    "        for i in range(levels):\n",
    "            layers.extend(self._make_simple_block(planes, stride = stride))\n",
    "        return layers\n",
    "            \n",
    "    def _build_list(self, x, layers):\n",
    "        for l in layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def _build_root(self, root, *x):\n",
    "        x = list(x)\n",
    "        conv = root[0]\n",
    "        bn = root[1]\n",
    "        act = root[2]\n",
    "        \n",
    "        x = concatenate(x, axis = self.norm_axis)\n",
    "        x = conv(x)\n",
    "        x = bn(x)\n",
    "        x = act(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def _build_tree(self,x, tree, children=None, residual=None):\n",
    "        if not isinstance(tree, Tree):\n",
    "            return self._build_list(x,tree)\n",
    "        \n",
    "        \n",
    "        children = [] if children is None else children\n",
    "        bottom = tree.downsample(x) if tree.downsample else x\n",
    "        residual = self._build_list(bottom,tree.project) if tree.project else bottom\n",
    "        \n",
    "        if tree.level_root:\n",
    "            children.append(bottom)\n",
    "\n",
    "        x1 = self._build_tree(x, tree.tree1, residual=residual)\n",
    "\n",
    "        if tree.levels == 1:\n",
    "            x2 = self._build_tree(x1,tree.tree2)\n",
    "            x = self._build_root(tree.root, x2, x1, *children)\n",
    "        else:\n",
    "            children.append(x1)\n",
    "            x = self._build_tree(x1, tree.tree2, children=children)\n",
    "        return x\n",
    "        \n",
    "    def build(self, inputs):\n",
    "        x = self._build_list(inputs, self.base_layer)\n",
    "        l0 = self._build_list(x, self.level0)\n",
    "        l1 = self._build_list(l0, self.level1)\n",
    "        l2 = self._build_tree(l1, self.level2)\n",
    "        l3 = self._build_tree(l2, self.level3)\n",
    "        l4 = self._build_tree(l3, self.level4)\n",
    "        l5 = self._build_tree(l4, self.level5)\n",
    "        return [l0,l1,l2,l3,l4,l5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DlaUp():\n",
    "    def __init__(self, planes, scales):\n",
    "        self.planes = planes\n",
    "        self.scales = scales\n",
    "    \n",
    "    def build(self, x):\n",
    "        # TODO\n",
    "        return x\n",
    "\n",
    "class DlaSeg():\n",
    "    def __init__(self, down_ratio=4, \n",
    "                 head_conv=256,\n",
    "                 input_shape = (512,512,3),  \n",
    "                 levels=[1, 1, 1, 2, 2, 1],\n",
    "                 planes=[16, 32, 64, 128, 256, 512],\n",
    "                 activation_function=\"relu\"):\n",
    "        # Params\n",
    "        assert down_ratio in [2, 4, 8, 16]\n",
    "        self.input = Input(shape = input_shape)\n",
    "        self.base = DlaKeras(levels=levels, planes=planes, activation_function=activation_function)\n",
    "        self.heads = heads={'hm': 1, 'wh': 2}\n",
    "        \n",
    "        self.first_level = int(np.log2(down_ratio))\n",
    "        scales = [2 ** i for i in range(len(planes[self.first_level:]))]\n",
    "        self.planes = planes\n",
    "        \n",
    "        # Up\n",
    "        self.dla_up = DlaUp(planes[self.first_level:], scales=scales)\n",
    "        \n",
    "        # Hed\n",
    "        self.head_layers = {}\n",
    "        self.head_conv = head_conv\n",
    "        for head in heads:\n",
    "            self.head_layers[head] = self.add_head(head)\n",
    "            \n",
    "    def add_head(self, head):\n",
    "        layers = []\n",
    "        classes = self.heads[head]\n",
    "        if self.head_conv > 0:\n",
    "            layers.append(Conv2D(self.head_conv, kernel_size=(3,3), padding=\"same\", use_bias=True))\n",
    "            layers.append(ReLU()),\n",
    "            \n",
    "        layers.append(Conv2D(classes, kernel_size=(1,1), strides=(1,1), padding='same', use_bias=True ))\n",
    "\n",
    "        return layers\n",
    "    \n",
    "    def _build_list(self, x, layers):\n",
    "        for l in layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "    \n",
    "    def build(self):\n",
    "        inputs = self.input\n",
    "        levels = self.base.build(inputs)\n",
    "        x = self.dla_up.build(levels[self.first_level:])\n",
    "       \n",
    "        # TODO\n",
    "        x = x[0]\n",
    "            \n",
    "        outputs = []\n",
    "        for head in self.head_layers:\n",
    "            outputs.append(self._build_list(x, self.head_layers[head])) # TODO x is temporart\n",
    "        return Model(inputs = inputs, outputs=outputs)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape calculated: (1, 128, 128, 1) (1, 128, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "m = DlaSeg().build()\n",
    "\n",
    "dummy = np.zeros((1,512,512,3), dtype=float)\n",
    "dummy_result = m.predict(dummy)\n",
    "dummy_hm, dummy_wh = dummy_result\n",
    "\n",
    "\n",
    "print(\"Output shape calculated:\", dummy_hm.shape, dummy_wh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy train: does it train at all ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Epoch 1/1\n",
      "12/12 [==============================] - 9s - loss: 59.8313 - conv2d_342_loss: 56.2605 - conv2d_344_loss: 3.5709    \n"
     ]
    }
   ],
   "source": [
    "def dummy_train(model:Model,n=12, epochs=1):\n",
    "    rand = np.random.rand(1,512,512,1)\n",
    "    dummy_inputs = np.repeat(np.repeat(rand, n, axis=0),3,axis=-1)\n",
    "    dummy_outputs_hm = np.zeros((n,dummy_hm.shape[1],dummy_hm.shape[2],dummy_hm.shape[3]))\n",
    "    dummy_outputs_wh = np.zeros((n,dummy_wh.shape[1],dummy_wh.shape[2],dummy_wh.shape[3]))\n",
    "    model.fit(dummy_inputs, [dummy_outputs_hm, dummy_outputs_wh], batch_size=4, epochs=epochs)\n",
    "    \n",
    "print(\"Training:\")\n",
    "m = DlaSeg().build()\n",
    "optimizer = SGD(lr=0.1)\n",
    "m.compile(optimizer=optimizer, loss='mse')\n",
    "dummy_train(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
